{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9a58ba",
   "metadata": {},
   "source": [
    "# Mountain Project Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a326bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "from re import match\n",
    "import time, os\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c31d15",
   "metadata": {},
   "source": [
    "### Get Links for Each Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb24b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup webdriver\n",
    "chromedriver = \"/Applications/chromedriver\" \n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a876b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785da6e8",
   "metadata": {},
   "source": [
    "### Get links of 1st page results from search (ratings 5.0-5.9)\n",
    "**Search filters:**<br>\n",
    "* Kentucky\n",
    "* 1+ avg stars \n",
    "* 5.0 - 5.9 \n",
    "\n",
    "Breaking up the search into different difficulty levels allows me to grab all the results, otherwise the search is limited to return 1,000 routes.\n",
    "\n",
    "The URL for the first page has a different pattern than the following pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a166874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab links from page 1 of results\n",
    "url = \"https://www.mountainproject.com/route-finder?type=rock&diffMinrock=1000&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=2500&diffMaxboulder=20050&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=1.8&pitches=0&selectedIds=105868674\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d840c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the first page url\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c144000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the soup object\n",
    "soup = bs(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962717b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff489d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "# search soup object to find table with links\n",
    "table = soup.find('table', class_='table table-striped route-table hidden-sm-up')\n",
    "\n",
    "# get all hyperlinks in table\n",
    "for link in table.find_all('a'):\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac406df6",
   "metadata": {},
   "source": [
    "### Get links for rest of results from search (ratings 5.0-5.9)\n",
    "**Search filters:**<br>\n",
    "* Kentucky\n",
    "* 1+ avg stars \n",
    "* 5.0 - 5.9\n",
    "\n",
    "This will be the links for pages 2 - 16 in the 5.0-5.9 category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# loop through each result page to grab html/links\n",
    "for i in range(2, 17):\n",
    "    url = \"https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=20050&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=2500&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=1000&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105868674&stars=1.8&type=rock&page=\" + str(i)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # create soup object for html\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # find the table with links in soup \n",
    "    table = soup.find('table', class_='table table-striped route-table hidden-sm-up')\n",
    "    \n",
    "    ## get all hyperlinks in table\n",
    "    for link in table.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    time.sleep(2)\n",
    "        \n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d82693",
   "metadata": {},
   "source": [
    "#### Get links for routes 5.10a - 5.10d\n",
    "This range of difficulty is very common, so the range is narrower to capture all routes.<br><br>\n",
    "**Search:** 5.10a - 5.10d<br>\n",
    "**Pages:** 1 - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e297f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#pg1 5.10a - 5.10d\n",
    "pg1 = \"https://www.mountainproject.com/route-finder?type=rock&diffMinrock=2600&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=3500&diffMaxboulder=20050&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=1.8&pitches=0&selectedIds=105868674\"\n",
    "driver.get(pg1)\n",
    "\n",
    "table = soup.find('table', \n",
    "                  class_='table table-striped route-table hidden-sm-up')\n",
    "\n",
    "for link in table.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "\n",
    "# loop through each result page to grab html/links\n",
    "for i in range(2, 14):\n",
    "    url = \"https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=20050&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=3500&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=2600&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105868674&stars=1.8&type=rock&page=\" + str(i)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # create soup object for html\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # find the table with links in soup \n",
    "    table = soup.find('table', class_='table table-striped route-table hidden-sm-up')\n",
    "    \n",
    "    ## get all hyperlinks in table\n",
    "    for link in table.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    time.sleep(2)\n",
    "        \n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcde52",
   "metadata": {},
   "source": [
    "#### Get links for routes 5.11a - 5.11d\n",
    "This range of difficulty is very common, so the range is narrower to capture all routes.<br><br>\n",
    "**Search:** 5.11a - 5.11d<br>\n",
    "**Pages:** 1 - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eee15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#pg1 5.10a - 5.10d\n",
    "pg1 = \"https://www.mountainproject.com/route-finder?type=rock&diffMinrock=4600&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=5500&diffMaxboulder=20050&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=1.8&pitches=0&selectedIds=105868674\"\n",
    "driver.get(pg1)\n",
    "\n",
    "table = soup.find('table', \n",
    "                  class_='table table-striped route-table hidden-sm-up')\n",
    "\n",
    "for link in table.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "\n",
    "# loop through each result page to grab html/links\n",
    "for i in range(2, 14):\n",
    "    url = \"https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=20050&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=5500&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=4600&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105868674&stars=1.8&type=rock&page=\" + str(i)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # create soup object for html\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # find the table with links in soup \n",
    "    table = soup.find('table', class_='table table-striped route-table hidden-sm-up')\n",
    "    \n",
    "    ## get all hyperlinks in table\n",
    "    for link in table.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    time.sleep(2)\n",
    "driver.close()        \n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76675e5",
   "metadata": {},
   "source": [
    "#### Get links for routes 5.11a - 5.11d\n",
    "\n",
    "**Search:** 5.12a - 5.15d<br>\n",
    "**Pages:** 1 - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4194a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#pg1\n",
    "pg1 = \"https://www.mountainproject.com/route-finder?type=rock&diffMinrock=6600&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=12400&diffMaxboulder=20050&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=1.8&pitches=0&selectedIds=105868674\"\n",
    "driver.get(pg1)\n",
    "\n",
    "table = soup.find('table', \n",
    "                  class_='table table-striped route-table hidden-sm-up')\n",
    "\n",
    "for link in table.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "\n",
    "# loop through each result page to grab html/links\n",
    "for i in range(2, 13):\n",
    "    url = \"https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=20050&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=6600&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105868674&stars=1.8&type=rock&page=\" + str(i)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # create soup object for html\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # find the table with links in soup \n",
    "    table = soup.find('table', class_='table table-striped route-table hidden-sm-up')\n",
    "    \n",
    "    ## get all hyperlinks in table\n",
    "    for link in table.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    time.sleep(2)\n",
    "driver.close()        \n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672e868",
   "metadata": {},
   "source": [
    "### Filter Links List\n",
    "We only want the links that are for the individual routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187315ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter links to include only the route links\n",
    "p = re.compile(r'https://www.mountainproject.com/route.*')\n",
    "routes_links = [i for i in links if p.match(i)]\n",
    "\n",
    "print(len(routes_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea93cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save routes_links to be used in another notebook\n",
    "import pickle\n",
    "\n",
    "pickle.dump(routes_links, open(\"routes_links.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37af87",
   "metadata": {},
   "source": [
    "### Scrape Each Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19c3e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store dictionaries of results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c748b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2539\n"
     ]
    }
   ],
   "source": [
    "# open driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# Loop through result links\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Dictionary for individual result\n",
    "    result = {}\n",
    "    \n",
    "    # Get name\n",
    "    try:\n",
    "        result['name'] = soup.find('h1').text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get location\n",
    "    try:\n",
    "        route_info = soup.find('div', id='route-page')\n",
    "        loc_info = route_info.find('div', \n",
    "                                   class_='mb-half small text-warm')\n",
    "        result['location'] = [link.text for link in loc_info.find_all('a')]\n",
    "        result['park'] = result['location'][-2]\n",
    "        result['wall'] = result['location'][-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get grade rating\n",
    "    try:\n",
    "        grade = soup.find('span', class_='rateYDS').text.split(' ')[0]\n",
    "        a = '1234567890.abcd'\n",
    "        grade = ''.join([i for i in grade if i in a])\n",
    "      \n",
    "        grade_dict = {'5.0':0,'5.1':1,'5.2':2,'5.3':3,'5.4':4,'5.5':5,\n",
    "                      '5.6':6,'5.7':7,'5.8':8,'5.9':9,'5.10a':10,\n",
    "                      '5.10b':11,'5.10c':12,'5.10d':13,'5.11a':14,'5.11b':15,\n",
    "                      '5.11c':16,'5.11d':17,'5.12a':18,'5.12b':19,'5.12c':20,\n",
    "                      '5.12d':21,'5.13a':22,'5.13b':23,'5.13c':24,'5.13d':25,\n",
    "                      '5.14a':26,'5.14b':27,'5.14c':28,'5.14d':29,'5.15a':30,\n",
    "                      '5.15b':31,'5.15c':32,'5.15d':33}\n",
    "        result['grade_score'] = [val for key,val in grade_dict.items() if key==grade][0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get details - style, length, pitches\n",
    "    try:\n",
    "        details = soup.find('table', class_='description-details')\n",
    "        \n",
    "        # Extract text\n",
    "        details_type = (details.find(text='Type:')\n",
    "                        .findNext().text.lower().strip().split(','))\n",
    "        \n",
    "        # Style\n",
    "        for item in details_type:\n",
    "            if details_type[0] == 'trad':\n",
    "                result['style'] = 'trad'\n",
    "            elif details_type[0] == 'sport':\n",
    "                result['style'] = 'sport'\n",
    "            elif details_type[0] == 'tr':\n",
    "                result['style'] = 'tr'\n",
    "        \n",
    "        # Length \n",
    "        result['route_length'] = (list(filter(lambda v: match(r'^\\s\\d+\\sft.*', v), \n",
    "                                              details_type))[0].strip().split(' ')[0])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # was getting error, hopefully just because pitches not listed, default 1\n",
    "    try:\n",
    "        # Pitches\n",
    "        result['num_pitches'] = (list(filter(lambda v: match(r'^.*\\d+\\spitches$', v), \n",
    "                                             details_type))[0].strip().split(' ')[0])\n",
    "    except:\n",
    "        result['num_pitches'] = 1\n",
    "    \n",
    "    # Get user-ratings\n",
    "    try:\n",
    "        stars_info = soup.find('a', class_='show-tooltip').text.strip().split(' ')\n",
    "        result['num_ratings'] = int(stars_info[3].replace(',', ''))\n",
    "        result['rating'] = float(stars_info[1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Shared by date \n",
    "    try:\n",
    "        shared_text = (details.find(text='Shared By:')\n",
    "                       .findNext().text.strip().split(' '))\n",
    "        result['share_date'] = [int(s) for s in shared_text if s.strip().isdigit()][0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Average page-views\n",
    "    try:\n",
    "        views_text = (details.find('td', class_='pr-half text-nowrap')\n",
    "                      .findNext().text.strip().split(' '))[-1]                   \n",
    "        month_views = [s for s in views_text if s.isdigit()]\n",
    "        result['month_views'] = int(''.join(month_views))\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Number of photos\n",
    "    try:\n",
    "        photos_text = (soup.find('button', id='more-photos-button')\n",
    "                       .text.strip().split(' '))\n",
    "        result['photos'] = [int(s) for s in photos_text if s.isdigit()][0] + 12\n",
    "    except:\n",
    "        try:\n",
    "            main = soup.find('div', class_='mt-3')\n",
    "            photos = main.find_all('a', class_='card-with-photo photo-card')\n",
    "            result['photos'] = int(len(photos)/2)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    # Number of comments\n",
    "    try:\n",
    "        comments_text = (soup.find('h2', class_='comment-count')\n",
    "                         .text.split(' '))\n",
    "        result['comments'] = [int(s) for s in comments_text if s.isdigit()][0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract the text in the details section\n",
    "    try:\n",
    "        main_info = soup.find(\n",
    "            'div', class_='col-md-9 main-content float-md-right')\n",
    "\n",
    "        info = [div.text for div in main_info.find_all('div', class_='fr-view')]\n",
    "        \n",
    "        # Description text\n",
    "        result['desc_len'] = len(info[0])\n",
    "        result['loc_len'] = len(info[1])\n",
    "        result['prot_len'] = len(info[2])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        stats_button = driver.find_element_by_xpath(\n",
    "            '//*[@id=\"route-star-avg\"]/span/a')\n",
    "        stats_button.click()\n",
    "        soup2 = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "        ticks = soup2.find(text='Ticks ')\n",
    "    \n",
    "        result['num_ticks'] = ticks.findNext().text\n",
    "    except:\n",
    "        result['num_ticks'] = 0\n",
    "    \n",
    "    \n",
    "    # add result dictionary to results list\n",
    "    results.append(result)\n",
    "driver.close()    \n",
    "print(len(results))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1731e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "987c1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle dataframe \n",
    "df.to_pickle(\"mtn_project_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422e4fb",
   "metadata": {},
   "source": [
    "### Scrape Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store dictionaries of results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# Loop through result links\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Dictionary for individual result\n",
    "    result = {}\n",
    "    \n",
    "    # Get name\n",
    "    try:\n",
    "        result['name'] = soup.find('h1').text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract the text in the details section\n",
    "    try:\n",
    "        main_info = soup.find(\n",
    "            'div', class_='col-md-9 main-content float-md-right')\n",
    "\n",
    "        info = [div.text for div in main_info.find_all('div', class_='fr-view')]\n",
    "        \n",
    "        # Description text\n",
    "        result['desc'] = info[0]\n",
    "        result['loc'] = info[1]\n",
    "        result['prot'] = info[2]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # add result dictionary to results list\n",
    "    results.append(result)\n",
    "driver.close()    \n",
    "print(len(results))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('proj_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665fca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat two dfs with an inner join\n",
    "df_combo = pd.concat([df, text_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8089083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove second index\n",
    "df_combo = df_combo.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate name column\n",
    "df_combo = df_combo.loc[:,~df_combo.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add area to df\n",
    "df_combo['area'] = df_combo['location'].apply(pd.Series)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3018c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo.to_csv('proj_mtn_df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
